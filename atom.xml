<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Leon Xu&#39;s Notes</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://github-9233.github.io/"/>
  <updated>2019-01-16T19:06:38.246Z</updated>
  <id>https://github-9233.github.io/</id>
  
  <author>
    <name>Leon Xu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Machine Learning Foundations &amp; Techniques</title>
    <link href="https://github-9233.github.io/2019/01/17/Machine%20Learning-Hsuan-Tien%20Lin-review/"/>
    <id>https://github-9233.github.io/2019/01/17/Machine Learning-Hsuan-Tien Lin-review/</id>
    <published>2019-01-16T16:00:00.000Z</published>
    <updated>2019-01-16T19:06:38.246Z</updated>
    
    <content type="html"><![CDATA[<p>Machine learning is the study that allows computers to adaptively improve their performance with experience accumulated from the data observed. Our two sister courses teach the most fundamental algorithmic, theoretical and practical tools that any user of machine learning needs to know. This first course of the two would focus more on mathematical tools, and the other course would focus more on algorithmic tools.</p><a id="more"></a><h1 id="machine-learning-foundations"><a href="https://www.csie.ntu.edu.tw/~htlin/course/" target="_blank" rel="noopener">Machine Learning Foundations</a></h1><h2 id="topic-1-when-can-machines-learn">Topic 1: When can machines learn?</h2><p>###Lecture 1: The Learning Problem</p><p><em>A</em> takes <em>D</em> and <em>H</em> to get <em>g</em></p><ul><li><p>Course Introduction</p><p>foundation oriented and story-like</p></li><li><p>What is Machine Learning</p><p>use data to approximate target</p></li><li><p>Applications of Machine Learning</p><p>almost everywhere</p></li><li><p>Components of Machine Learning</p><p><em>A</em> takes <em>D</em> and <em>H</em> to get <em>g</em></p></li><li><p>Machine Learning and Other Fields</p><p>related to Date Mining, Artificial Intelligence and Statstics</p></li></ul><h3 id="lecture-2-learning-to-answer-yesno">Lecture 2: Learning to Answer Yes/No</h3><p>PLA <em>A</em> takes linear separable <em>D</em> and perceptrons <em>H</em> to get hypothesis <em>g</em></p><ul><li><p>Perceptron Hypothesis Set</p><p>hyperplanes/linear classifiers in Rd</p></li><li><p>Perceptron Learning Algorithm (PLA)</p><p>correct mistakes and improve iteratively</p></li><li><p>Guarantee of PLA</p><p>no mistake eventually if linear separable</p></li><li><p>Non-Separable Data</p><p>hold somewhat ‘best’ weights in pocket</p></li></ul><h3 id="lecture-3-types-of-learning">Lecture 3: Types of Learning</h3><p>focus: binary classification or regression from a batch of supervised data with concrete features</p><ul><li><p>Learning with Different Output Space Y</p><p>[classification], [regression], structured</p></li><li><p>Learning with Different Data Label yn</p><p>[supervised], un/semi-supervised, reinforcement</p></li><li><p>Learning with Different Protocol f ⇒ (xn, yn)</p><p>[batch], online, active</p></li><li><p>Learning with Different Input Space X</p><p>[concrete], raw, abstract</p></li></ul><h3 id="lecture-4-feasibility-of-learning">Lecture 4: feasibility of learning</h3><p>learning is PAC-possible if enough statistical data and finite |H|</p><ul><li><p>Learning is Impossible?</p><p>absolutely no free lunch outside D</p></li><li><p>Probability to the Rescue</p><p>probably approximately correct outside D</p></li><li><p>Connection to Learning</p><p>verification possible if Ein(h) small for fixed h</p></li><li><p>Connection to Real Learning</p><p>learning possible if |H| finite and Ein(g) small</p></li></ul><h2 id="topic-2-why-can-machines-learn">Topic 2: Why Can Machines Learn?</h2><h3 id="lecture-5-training-versus-testing">Lecture 5: training versus testing</h3><p>effective price of choice in training: (wishfully) growth function mH(N) with a break point</p><ul><li><p>Recap and Preview</p><p>two questions: Eout(g) ≈ Ein(g), and Ein(g) ≈ 0</p></li><li><p>Effective Number of Lines</p><p>at most 14 through the eye of 4 inputs</p></li><li><p>Effective Number of Hypotheses</p><p>at most mH(N) through the eye of N inputs</p></li><li><p>Break Point</p><p>when mH(N) becomes ‘non-exponential’</p></li></ul><h3 id="lecture-6-theory-of-generalization">Lecture 6: theory of generalization</h3><p>Eout ≈ Ein possible if mH(N) breaks somewhere and N large enough</p><ul><li><p>Restriction of Break Point</p><p>break point ‘breaks’ consequent points</p></li><li><p>Bounding Function: Basic Cases</p><p>B(N,k) bounds mH(N) with break point k</p></li><li><p>Bounding Function: Inductive Cases</p><p>B(N,k) is poly(N)</p></li><li><p>A Pictorial Proof</p><p>mH(N) can replace M with a few changes</p></li></ul><h3 id="lecture-7-the-vc-dimension">Lecture 7: the VC dimension</h3><p>learning happens if finite dVC, large N, and low Ein</p><ul><li><p>Definition of VC Dimension</p><p>maximum non-break point</p></li><li><p>VC Dimension of Perceptrons</p><p>dVC(H) = d + 1</p></li><li><p>Physical Intuition of VC Dimension</p><p>dVC ≈ #free parameters</p></li><li><p>Interpreting VC Dimension</p><p>loosely: model complexity &amp; sample complexity</p></li></ul><h3 id="lecture-8-noise-and-error">Lecture 8: noise and error</h3><p>learning can happen with target distribution P(y|x) and low Ein w.r.t. err</p><ul><li><p>Noise and Probabilistic Target</p><p>can replace f(x) by P(y|x)</p></li><li><p>Error Measure</p><p>affect ‘ideal’ target</p></li><li><p>Algorithmic Error Measure</p><p>user-dependent =⇒ plausible or friendly</p></li><li><p>Weighted Classification</p><p>easily done by virtual ‘example copying’</p></li></ul><h2 id="topic-3-how-can-machines-learn">Topic 3: How Can Machines Learn?</h2><h3 id="lecture-9-linear-regression">Lecture 9: linear regression</h3><p>analytic solution wLIN = X†y with linear regression hypotheses and squared error</p><ul><li><p>Linear Regression Problem</p><p>use hyperplanes to approximate real values</p></li><li><p>Linear Regression Algorithm</p><p>analytic solution with pseudo-inverse</p></li><li><p>Generalization Issue</p><p>Eout − Ein ≈ 2(d+1) on average N</p></li><li><p>Linear Regression for Binary Classification</p><p>0/1 error ≤ squared error</p></li></ul><h3 id="lecture-10-logistic-regression">Lecture 10: logistic regression</h3><p>gradient descent on cross-entropy error to get good logistic hypothesis</p><ul><li><p>Logistic Regression Problem</p><p>P(+1|x) as target and θ(wT x) as hypotheses</p></li><li><p>Logistic Regression Error</p><p>cross-entropy (negative log likelihood)</p></li><li><p>Gradient of Logistic Regression Error</p><p>θ-weighted sum of data vectors</p></li><li><p>Gradient Descent</p><p>roll downhill by −∇Ein(w)</p></li></ul><h3 id="lecture-11-linear-models-for-classification">Lecture 11: linear models for classification</h3><p>binary classification via (logistic) regression; multiclass via OVA/OVO decomposition</p><ul><li><p>Linear Models for Binary Classification</p><p>three models useful in different ways</p></li><li><p>Stochastic Gradient Descent</p><p>follow negative stochastic gradient</p></li><li><p>Multiclass via Logistic Regression</p><p>predict with maximum estimated P(k|x)</p></li><li><p>Multiclass via Binary Classification</p><p>predict the tournament champion</p></li></ul><h3 id="lecture-12-nonlinear-transformation">Lecture 12: nonlinear transformation</h3><p>nonlinear X􏰡 via nonlinear feature transform Φ plus linear X with price of model complexity</p><ul><li><p>Quadratic Hypotheses</p><p>linear hypotheses on quadratic-transformed data</p></li><li><p>Nonlinear Transform</p><p>happy linear modeling after Z = Φ(X )</p></li><li><p>Price of Nonlinear Transform</p><p>computation/storage/[model complexity]</p></li><li><p>Structured Hypothesis Sets</p><p>linear/simpler model first</p></li></ul><h2 id="topic-4-how-can-machines-learn-better">Topic 4: How Can Machines Learn Better?</h2><h3 id="lecture-13-hazard-of-overfitting">Lecture 13: hazard of overfitting</h3><p>overfitting happens with excessive power, stochastic/deterministic noise, and limited data</p><ul><li><p>What is Overfitting?</p><p>lower Ein but higher Eout</p></li><li><p>The Role of Noise and Data Size</p><p>overfitting ‘easily’ happens!</p></li><li><p>Deterministic Noise</p><p>what H cannot capture acts like noise</p></li><li><p>Dealing with Overfitting</p><p>data cleaning/pruning/hinting, and more</p></li></ul><h3 id="lecture-14-regularization">Lecture 14: regularization</h3><p>minimizes augmented error, where the added regularizer effectively limits model complexity</p><ul><li><p>Regularized Hypothesis Set</p><p>original H + constraint</p></li><li><p>Weight Decay Regularization</p><p>add λ/N*wTw in Eaug</p></li><li><p>Regularization and VC Theory</p><p>regularization decreases dEFF</p></li><li><p>General Regularizers</p><p>target-dependent, [plausible], or [friendly]</p></li></ul><h3 id="lecture-15-validation">Lecture 15: validation</h3><p>(crossly) reserve validation data to simulate testing procedure for model selection</p><ul><li><p>Model Selection Problem</p><p>dangerous by Ein and dishonest by Etest</p></li><li><p>Validation</p><p>select with Eval(Am(Dtrain)) while returning Am∗ (D)</p></li><li><p>Leave-One-Out Cross Validation</p><p>huge computation for almost unbiased estimate</p></li><li><p>V -Fold Cross Validation</p><p>reasonable computation and performance</p></li></ul><h3 id="lecture-16-three-learning-principle">Lecture 16: three learning principle</h3><ul><li><p>Occam’s Razor</p><p>simple, simple, simple!</p></li><li><p>Sampling Bias</p><p>match test scenario as much as possible</p></li><li><p>Data Snooping</p><p>any use of data is ‘contamination’</p></li><li><p><strong>Power of Three</strong></p><p>relatives, bounds, models, tools, principles</p></li></ul><h1 id="machine-learning-techniques">Machine Learning Techniques</h1><h2 id="topic-1-embedding-numerous-features-kernel-models">Topic 1: Embedding Numerous Features: Kernel Models</h2><h3 id="lecture-1-linear-support-vector-machine">Lecture 1: Linear Support Vector Machine</h3><p>linear SVM: more robust and solvable with quadratic programming</p><ul><li><p>Course Introduction</p><p>from foundations to techniques</p></li><li><p>Large-Margin Separating Hyperplane</p><p>intuitively more robust against noise</p></li><li><p>Standard Large-Margin Problem</p><p>minimize ‘length of w’ at special separating scale</p></li><li><p>Support Vector Machine</p><p>‘easy’ via quadratic programming</p></li><li><p>Reasons behind Large-Margin Hyperplane</p><p>fewer dichotomies and better generalization</p></li></ul><h3 id="lecture-2-dual-support-vector-machine">Lecture 2: Dual Support Vector Machine</h3><p>dual SVM: another QP with valuable geometric messages and almost no dependence on d ̃</p><ul><li><p>Motivation of Dual SVM</p><p>want to remove dependence on d ̃</p></li><li><p>Lagrange Dual SVM</p><p>KKT conditions link primal/dual</p></li><li><p>Solving Dual SVM</p><p>another QP, better solved with special solver</p></li><li><p>Messages behind Dual SVM</p><p>SVs represent fattest hyperplane</p></li></ul><h3 id="lecture-3-kernel-support-vector-machine">Lecture 3: Kernel Support Vector Machine</h3><p>kernel as a shortcut to (transform + inner product) to remove dependence on d ̃: allowing a spectrum of simple (linear) models to infinite dimensional (Gaussian) ones with margin control</p><ul><li><p>Kernel Trick</p><p>kernel as shortcut of transform + inner product</p></li><li><p>Polynomial Kernel</p><p>embeds specially-scaled polynomial transform</p></li><li><p>Gaussian Kernel</p><p>embeds infinite dimensional transform</p></li><li><p>Comparison of Kernels</p><p>linear for efficiency or Gaussian for power</p></li></ul><h3 id="lecture-4-soft-margin-support-vector-machine">Lecture 4: Soft-Margin Support Vector Machine</h3><p>allow some margin violations ξn while penalizing them by C; equivalent to upper-bounding αn by C</p><ul><li><p>Motivation and Primal Problem</p><p>add margin violations ξn</p></li><li><p>Dual Problem</p><p>upper-bound αn by C</p></li><li><p>Messages behind Soft-Margin SVM</p><p>bounded/free SVs for data analysis</p></li><li><p>Model Selection</p><p>cross-validation, or approximately nSV</p></li></ul><h3 id="lecture-5-kernel-logistic-regression">Lecture 5: Kernel Logistic Regression</h3><p>two-level learning for SVM-like sparse model for soft classification, or using representer theorem with regularized logistic error for dense model</p><ul><li><p>Soft-Margin SVM as Regularized Model</p><p>L2-regularization with hinge error measure</p></li><li><p>SVM versus Logistic Regression</p><p>≈ L2-regularized logistic regression</p></li><li><p>SVM for Soft Binary Classification</p><p>common approach: two-level learning</p></li><li><p>Kernel Logistic Regression</p><p>representer theorem on L2-regularized LogReg</p></li></ul><h3 id="lecture-6-support-vector-regression">Lecture 6: Support Vector Regression</h3><p>kernel ridge regression (dense) via ridge regression + representer theorem; support vector regression (sparse) via regularized tube error + Lagrange dual</p><ul><li><p>Kernel Ridge Regression</p><p>representer theorem on ridge regression</p></li><li><p>Support Vector Regression Primal</p><p>minimize regularized tube errors</p></li><li><p>Support Vector Regression Dual</p><p>a QP similar to SVM dual</p></li><li><p>Summary of Kernel Models</p><p>with great power comes great responsibility</p></li></ul><h2 id="topic-2-combining-predictive-features-aggregation-models">Topic 2: Combining Predictive Features: Aggregation Models</h2><h3 id="lecture-7-blending-and-bagging">Lecture 7: Blending and Bagging</h3><p>blending known diverse hypotheses uniformly, linearly, or even non-linearly; obtaining diverse hypotheses from bootstrapped data</p><ul><li><p>Motivation of Aggregation</p><p>aggregated G strong and/or moderate</p></li><li><p>Uniform Blending</p><p>diverse hypotheses, ‘one vote, one value’</p></li><li><p>Linear and Any Blending</p><p>two-level learning with hypotheses as transform</p></li><li><p>Bagging (Bootstrap Aggregation)</p><p>bootstrapping for diverse hypotheses</p></li></ul><h3 id="lecture-8-adaptive-boosting">Lecture 8: Adaptive Boosting</h3><p>optimal re-weighting for diverse hypotheses and adaptive linear aggregation to boost ‘weak’ algorithms</p><ul><li><p>Motivation of Boosting</p><p>aggregate weak hypotheses for strength</p></li><li><p>Diversity by Re-weighting</p><p>scale up incorrect, scale down correct</p></li><li><p>Adaptive Boosting Algorithm</p><p>two heads are better than one, theoretically</p></li><li><p>Adaptive Boosting in Action</p><p>AdaBoost-Stump useful and efficient</p></li></ul><h3 id="lecture-9-decision-tree">Lecture 9: Decision Tree</h3><p>recursive branching (purification) for conditional aggregation of constant hypotheses</p><ul><li><p>Decision Tree Hypothesis</p><p>express path-conditional aggregation</p></li><li><p>Decision Tree Algorithm</p><p>recursive branching until termination to base</p></li><li><p>Decision Tree Heuristics in C&amp;RT</p><p>pruning, categorical branching, surrogate</p></li><li><p>Decision Tree in Action</p><p>explainable and efficient</p></li></ul><h3 id="lecture-10-random-forest">Lecture 10: Random Forest</h3><p>bagging of randomized C&amp;RT trees with automatic validation and feature selection</p><ul><li><p>Random Forest Algorithm</p><p>bag of trees on randomly projected subspaces</p></li><li><p>Out-Of-Bag Estimate</p><p>self-validation with OOB examples</p></li><li><p>Feature Selection</p><p>permutation test for feature importance</p></li><li><p>Random Forest in Action</p><p>‘smooth’ boundary with many trees</p></li></ul><h3 id="lecture-11-gradient-boosted-decision-tree">Lecture 11: Gradient Boosted Decision Tree</h3><p>aggregating trees from functional gradient and steepest descent subject to any error measure</p><ul><li><p>Adaptive Boosted Decision Tree</p><p>sampling and pruning for ‘weak’ trees</p></li><li><p>Optimization View of AdaBoost</p><p>functional gradient descent on exponential error</p></li><li><p>Gradient Boosting</p><p>iterative steepest residual fitting</p></li><li><p>Summary of Aggregation Models</p><p>some cure underfitting; some cure overfitting</p></li></ul><h2 id="topic-3-distilling-implicit-features-extraction-models">Topic 3: Distilling Implicit Features: Extraction Models</h2><h3 id="lecture-12-neural-network">Lecture 12: Neural Network</h3><p>automatic pattern feature extraction from layers of neurons with backprop for GD/SGD</p><ul><li><p>Motivation</p><p>multi-layer for power with biological inspirations</p></li><li><p>Neural Network Hypothesis</p><p>layered pattern extraction until linear hypothesis</p></li><li><p>Neural Network Learning</p><p>backprop to compute gradient efficiently</p></li><li><p>Optimization and Regularization</p><p>tricks on initialization, regularizer, early stopping</p></li></ul><h3 id="lecture-13-deep-learning">Lecture 13: Deep Learning</h3><p>pre-training with denoising autoencoder (non-linear PCA) and fine-tuning with backprop for NNet with many layers</p><ul><li><p>Deep Neural Network</p><p>difficult hierarchical feature extraction problem</p></li><li><p>Autoencoder</p><p>unsupervised NNet learning of representation</p></li><li><p>Denoising Autoencoder</p><p>using noise as hints for regularization</p></li><li><p>Principal Component Analysis</p><p>linear autoencoder variant for data processing</p></li></ul><h3 id="lecture-14-radial-basis-function-network">Lecture 14: Radial Basis Function Network</h3><p>linear aggregation of distance-based similarities using k-Means clustering for prototype finding</p><ul><li>RBF Network Hypothesis</li></ul><p>prototypes instead of neurons as transform</p><ul><li>RBF Network Learning</li></ul><p>linear aggregation of prototype ‘hypotheses’</p><ul><li>k-Means Algorithm</li></ul><p>clustering with alternating optimization</p><ul><li>k-Means and RBF Network in Action</li></ul><p>proper choice of # prototypes important</p><h3 id="lecture-15-matrix-factorization">Lecture 15: Matrix Factorization</h3><p>linear models of movies on extracted user features (or vice versa) jointly optimized with stochastic gradient descent</p><ul><li>Linear Network Hypothesis</li></ul><p>feature extraction from binary vector encoding</p><ul><li>Basic Matrix Factorization</li></ul><p>alternating least squares between user/movie</p><ul><li>Stochastic Gradient Descent</li></ul><p>efficient and easily modified for practical use</p><ul><li>Summary of Extraction Models</li></ul><p>powerful thus need careful use</p><h3 id="lecture-16-finale">Lecture 16: Finale</h3><ul><li><p>Feature Exploitation Techniques</p><p>kernel, aggregation, extraction, low-dimensional</p></li><li><p>Error Optimization Techniques</p><p>gradient, equivalence, stages</p></li><li><p>Overfitting Elimination Techniques</p><p>(lots of) regularization, validation</p></li><li><p>Machine Learning in Practice</p><p>welcome to the jungle</p></li></ul><h1 id="notes">Notes</h1><ul><li><p>Terms</p><ul><li>正则 岭回归</li><li>伪逆矩阵</li><li>离散函数</li><li>拉格朗日对偶法</li><li>拉格朗日最小化</li><li>拉格朗日乘法</li><li>空间转换</li><li>原始SVM</li><li>对偶SVM</li><li>Decision Stump</li></ul></li><li><p>L1-5 25</p></li><li><p>L2-19</p></li><li><p>L4-18 19</p></li><li><p>L5-4 5 9 15 17</p></li><li><p>L7基于以上的两个例子，我们得到了aggregation的两个优势：feature transform和regularization。我们之前在机器学习基石课程中就介绍过，feature transform和regularization是对立的，还把它们分别比作踩油门和踩刹车。如果进行feature transform，那么regularization的效果通常很差，反之亦然。也就是说，单一模型通常只能倾向于feature transform和regularization之一，在两者之间做个权衡。但是aggregation却能将feature transform和regularization各自的优势结合起来，好比把油门和刹车都控制得很好，从而得到不错的预测模型。</p></li><li><p>L07-20 bootstrap aggregation (BAGging): a simple meta algorithm on top of base algorithm A</p></li><li><p>L07-21 值得注意的是，只有当演算法对数据样本分布比较敏感的情况下，才有比较好的表现。</p></li><li><p>L08-20</p></li><li><p>L09-10</p></li><li><p>L10-2Bagging能减小variance，而Decision Tree能增大variance</p></li><li><p>L11-10</p></li><li><p>L12-6感知机模型每个节点的输入就对应神经元的树突dendrite，感知机每个节点的输出就对应神经元的轴突axon</p></li><li><p>L12-13 16GD 21</p></li><li><p>L14-5 9 10</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Machine learning is the study that allows computers to adaptively improve their performance with experience accumulated from the data observed. Our two sister courses teach the most fundamental algorithmic, theoretical and practical tools that any user of machine learning needs to know. This first course of the two would focus more on mathematical tools, and the other course would focus more on algorithmic tools.&lt;/p&gt;
    
    </summary>
    
      <category term="Computer science" scheme="https://github-9233.github.io/categories/Computer-science/"/>
    
      <category term="Machine learning" scheme="https://github-9233.github.io/categories/Computer-science/Machine-learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Myscope</title>
    <link href="https://github-9233.github.io/2019/01/11/Microscopy/"/>
    <id>https://github-9233.github.io/2019/01/11/Microscopy/</id>
    <published>2019-01-11T13:11:14.000Z</published>
    <updated>2019-04-11T01:47:29.532Z</updated>
    
    <content type="html"><![CDATA[<p>MyScope was developed by Microscopy Australia to provide an online learning environment for those who want to learn about microscopy. The platform provides insights into the fundamental science behind different microscopes, explores what can and cannot be measured by different systems and provides a realistic operating experience on high end microscopes.</p><a id="more"></a><h2 id="microscopy-basics"><a href="https://myscope.training/index.html#" target="_blank" rel="noopener">Microscopy Basics</a></h2><h3 id="compound-microscope">Compound Microscope</h3><ul><li><p>eyepiece</p></li><li><p>objective lense</p></li></ul><h3 id="magnification">Magnification</h3><ul><li>max magnification ~ 500 to 1000x</li></ul><h3 id="resolution">Resolution</h3><ul><li><p>resolving Power depends:</p><ul><li><p>light gathering power of the objective lens through the medium the light passes through (<strong>Numerical Aperture</strong>, NA)</p><p>larger NA =&gt; greater resolution</p></li><li><p>wavelength of illuminating light</p><p>lower wavelength =&gt; greater resolution</p></li></ul></li><li><p>resolution limit</p><ul><li>~ 200 nm (Optical Microscopy)</li><li>~ 0.25 nm (Electron Microscopes)</li></ul></li></ul><h3 id="illumination-systems">Illumination Systems</h3><figure><img src="/2019/01/11/Microscopy/Users/xsx/Desktop/source/photos/figure_04.svg" alt="figure_04"><figcaption>figure_04</figcaption></figure><ul><li>FM Radio signals wavelengths: ~ 3 metres</li><li>visible spectrum wavelengths: ~ 400 nm (blue) to ~ 700 nm(red)</li><li><strong><em>Confocal system</em></strong></li></ul><h3 id="microscope-components">Microscope Components</h3><ul><li>Base to give stability: Usually contains the illumination components.</li><li>Stand: For moving and stability; locates all other parts.</li><li>Binocular head: To hold eyepieces and a trinocular head for an optional camera.</li><li>Revolving turret with objective lenses: For selecting various object magnifications</li><li>Stage: To hold specimen slide including stage motion controls.</li><li>Condenser under the stage with a diaphragm: To control the illumination.</li></ul><h3 id="microscope-designs">Microscope Designs</h3><ul><li>reflective<ul><li>stereoscopic</li><li>upright</li><li>scanning electron microscopes</li></ul></li><li>transmitted<ul><li>stereoscopic</li><li>upright</li><li>transmitted electron microscopes</li></ul></li></ul><h3 id="optical-microscopy-basics">Optical Microscopy Basics</h3><p><strong>Optical Microscopy</strong> is a term that broadly covers a wide range of light microscopy methods</p><ul><li>light microscopy methods<ul><li>transmitted light</li><li><strong>contrasting methods</strong><ul><li><em>phase contrast</em></li><li><em>dark-field</em></li><li><em>polarisation</em></li><li><em>differential interference contrast (DIC)microscopy</em></li></ul></li><li><em>fluorescence</em></li><li><em>laser microscopy</em></li></ul></li></ul><h3 id="electron-microscopy-basics">Electron Microscopy Basics</h3><ul><li>transmitted electron microscopy (TEM)</li><li>scanning electron microscopy (SEM)</li><li>environmental scanning electron (ESEM)</li><li>field emision-scanning electron microscopy (FE-SEM)</li><li>field emision transmission electron microscopy (FE-TEM)</li><li><em>Micro-probe EM</em></li></ul><h3 id="advanced-electron-microscopy">Advanced Electron Microscopy</h3><ul><li><p>Energy dispersive spectroscopy EDS (or Energy Dispersive X-ray spectroscopy EDX)</p></li><li><p>Wavelength Dispersive Spectroscopy (WDS) (or wavelength dispersive X-ray spectroscopy)</p><p>WDS is used in conjunction with SEM and Electron Probe.</p></li><li><p><em>SEM-CL:cathodoluminescence</em></p></li><li><p>Serial-section Scanning Electron Microscopy</p></li><li><p>Environmental Scanning Electron Microscopy (ESEM)</p></li><li><p><em>Focused Ion Beam Microscopy (FIB)</em></p></li></ul><h3 id="advanced-optical-microscopy">Advanced Optical Microscopy</h3><ul><li><p>Fluorescence</p></li><li><p><em>Confocal microscopy</em></p></li><li><p>Digital microscopes</p></li></ul><h3 id="microscopy-and-measurement">Microscopy and Measurement</h3><ul><li><strong><em>Field Number (FN)</em></strong></li><li>more accurate estimate<ul><li>stage micrometers</li><li>ocular reticules</li></ul></li><li>calibration</li></ul><h3 id="photomicrography">Photomicrography</h3><p><em>The objective on the microscope is the lens for the camera. This means the aperture is that of the objective. As a result the camera must be set to aperture priority mode.</em></p><ul><li><p>Pixels</p></li><li><p>Data Processing</p></li><li><p><em>Binning Data</em></p></li></ul><h3 id="image-management">Image Management</h3><h4 id="key-considerations-in-modern-digital-systems">Key Considerations In Modern Digital Systems</h4><ul><li>Whether the image is colour or monochrome</li><li>Size of the chip</li><li>Pixel size</li><li>Quantum efficiency</li><li>Readout noise</li><li>Dynamic range in which the image is stored</li><li>Bit depth</li><li>Frame rate at data collection</li><li>Pixel number</li><li>Cooling</li><li>Gain or binning</li><li>Whether the camera is designed for a microscope or consumer use</li><li>Monitor quality</li></ul><h4 id="reconstruction">Reconstruction</h4><ul><li><p>2D to 3D: <em>Filtered Backprojection</em></p></li><li><p>3D to Visual Render: voxels (VOlume (X) ELements - similar to a pixel)</p></li></ul><h3 id="image-ethics-and-manipulation">Image Ethics and Manipulation</h3><h2 id="scanning-electron-microscopy">Scanning Electron Microscopy</h2><h3 id="what-is-sem">What is SEM?</h3><h4 id="background-information">Background information</h4><ul><li><p>magnification: ~10 - 300,000</p></li><li><p>applications: materials science, biological science, geology, medical science and forensic science</p><ul><li>optoelectronic behaviour of semiconductors by <strong>cathodoluminescence [CL]</strong></li><li><em>Image molecular probes by using metal and fluorescent probes in biological samples.</em></li><li>grain orientation/crystallographic orientation by <strong>EBSD</strong></li></ul></li><li><p>Difference to a light microscope</p><ul><li>Resolution at high-magnification<ul><li>LM ~ 200 nm</li><li>SEM ~ 5 nm</li></ul></li><li>Depth of field: SEM &gt; 300 times LM</li><li>Microanalysis</li></ul></li></ul><h4 id="what-cant-it-do">What can't it do?</h4><ul><li><p>Image wet samples</p></li><li><p>Image non-conductive samples</p></li><li><p>Colour images</p></li><li><p>Accurate height measurement</p></li><li><p>Sub-surface imaging</p></li><li><p>Imaging through fluid</p></li><li><p>Atomic imaging</p></li><li><p><strong><em>Elemental analysis below micrometre scale</em></strong></p><p><em>Elemental analysis of areas less than 1 micrometre can be very difficult in an SEM. This is due to the interaction volume between the electron beam and the sample which is often in the micrometre range. The interaction volume can be reduced by reducing the electron beam accelerating voltage. However, the corresponding reduction in signal can make it difficult to acquire useful data.</em></p></li><li><p>Image charged molecules</p></li></ul><h3 id="how-does-an-sem-work">How does an SEM work?</h3><h4 id="the-electron-gun">The electron gun</h4><figure><img src="/2019/01/11/Microscopy/Users/xsx/Desktop/source/photos/figure_13.svg" alt="figure_13"><figcaption>figure_13</figcaption></figure><h5 id="electron-source">Electron source</h5><ul><li><p>thermionic (filament + Wehnelt cap + anode): filament current</p><ul><li>W(tungsten hairpin)</li><li>LaB6(single/multicrystal)</li></ul></li><li><p>field emission:</p><ul><li><p>cold field emission gun: extraction voltage</p><ul><li><p><strong>advantages</strong> smaller source size, high current, high brightness, low energy spread, long life</p></li><li><p><strong>disadvantages</strong> most coherent source=&gt; least appropriate for energy dispersive X-ray analysis</p></li></ul></li><li><p>hot field emission (Schottky field emission) gun:</p><ul><li><strong>advantages</strong>:<ul><li>better beam current stability, less stringent vacuum requirements and there is no need for periodic emitter flashing (heating the cold filament for a short time each day) to restore the emission current.</li><li>high beam current (&gt;100nA) with little decrease in spatial resolution</li></ul></li></ul></li></ul></li></ul><figure><img src="/2019/01/11/Microscopy/Users/xsx/Desktop/source/photos/figure_15.svg" alt="figure_15"><figcaption>figure_15</figcaption></figure><h5 id="filament-saturation">Filament saturation</h5><ul><li><p>amount of current</p></li><li><p>current stability</p></li></ul><h4 id="vacuum-system">Vacuum system</h4><h5 id="types-of-pumps"><a href="https://myscope.training/index.html#/SEMlevel_3_8" target="_blank" rel="noopener">Types of pumps</a></h5><ul><li><p><em>Rotary pumps</em></p></li><li><p><em>Diffusion pumps</em></p></li><li><p><em>Scroll pumps</em></p></li><li><p><em>Turbo-molecular pumps</em></p></li><li><p><em>Ion getter pumps</em></p></li></ul><h5 id="vacuum-requirements">Vacuum requirements</h5><ul><li><p>high-vacuum mode: SE mode &amp; secondary electrons</p></li><li><p>low-vacuum mode: backscattered electrons &amp; characteristic X-rays</p></li></ul><h4 id="water-chilling-system">water chilling system</h4><h4 id="structure-of-the-column">Structure of the column</h4><ul><li><p>condenser lens</p></li><li><p>objective lens</p></li><li><p>scanning coils (two pairs of electromagnetic deflection coils)</p><ul><li><em>bends the beam off the optical axis of the microscope</em></li><li><em>bends the beam back onto the axis at the pivot point of the scan</em></li></ul></li></ul><h4 id="specimen-chamber">Specimen chamber</h4><ul><li><p>Stage</p></li><li><p>Detectors</p><ul><li><p>Secondary electron detector (SE) (<em>Everhart-Thornley detector</em> &amp; <em>in-lens SE detector</em>)</p><ul><li><p>low energies (~2 to 50 eV)</p></li><li><p>small bias (~ +200 to 300V) to collect SE</p></li></ul></li><li><p>Backscattered electron detector (BSD)</p><ul><li><p><em>mounted below the objective lens pole piece and centred around the optic axis</em></p></li><li><p>higher energies than SE</p></li></ul></li><li><p>X-ray detectors</p><ul><li><p>energy dispersive X-ray spectrometry (EDS)</p></li><li><p><em>The EDS detector is based on a semiconductor crystal. The two most common types are the lithium-drifted silicon (SiLi) and the silicon drift detector (SDD).</em></p></li></ul></li><li><p><em>CL (cathodoluminescence) detectors</em></p></li></ul></li></ul><h4 id="beamspecimen-interactions">Beam/specimen interactions</h4><ul><li>regions<ul><li>SE (top 15nm)</li><li>BSE (top 40%)</li><li>X rays (entire region)</li></ul></li></ul><figure><img src="/2019/01/11/Microscopy/Users/xsx/Desktop/source/photos/figure_26.gif" alt="figure_26"><figcaption>figure_26</figcaption></figure><ul><li><p>Electron-matter interactions</p><ul><li>Elastic scattering: BSE</li><li>Inelastic scattering<ul><li>phonon excitation (heating)</li><li>cathodoluminescence (visible light fluorescence)</li><li><em>continuum radiation (bremsstrahlung)</em></li><li>characteristic X-ray radiation</li><li><em>plasmon production (secondary electrons)</em></li><li><em>Auger electrons (ejection of outer shell electrons)</em></li></ul></li></ul></li><li><p>Modelling interactions</p><ul><li><em>Casino (Monte Carlo Simulation of electroN trajectory in sOlids)</em></li></ul></li></ul><h4 id="the-image">The image</h4><ul><li><p>SE images(surface of sample)</p><ul><li>increase the yield of SE &lt;= coat thin layer(~10 nm) heavy metals (gold/platinum)<ul><li>non-conductive coated to reduce surface charging</li><li>low atomic number (Z) specimens (e.g. biological samples) coated to provide a surface layer</li></ul></li></ul></li><li><p>The contribution of BSE to images collected with the SE detector</p><ul><li><p>higher kV =&gt; sub-surface information due to various backscattered effects (elastic scattering)</p><ul><li><p>SE1: interaction between primary electron beam and sample</p></li><li><p>SE2: generate by BSE</p></li><li><p>SE3: interaction of beam with sample chamber, pole piece etc</p></li></ul></li></ul></li></ul><h3 id="how-do-i-get-a-good-image">How do I get a good image?</h3><h4 id="a-basic-guide-to-using-an-sem">A basic guide to using an SEM</h4><figure><img src="/2019/01/11/Microscopy/Users/xsx/Desktop/source/photos/下载.png" alt="下载"><figcaption>下载</figcaption></figure><h4 id="specimen-preparation">Specimen preparation</h4><ul><li><p>Biological samples</p><ul><li>fixation =&gt; dehydration =&gt; drying =&gt; coated conductive material</li></ul></li><li><p>Polymer samples</p></li><li><p>Non-biological samples</p></li><li><p>Mounting the sample</p><ul><li>SEM samples are attached to a support called a <strong>stub</strong></li><li>There <strong>must</strong> be a continuous electrical connection between the stub and the sample so that the charge does not build up. (conductive tapes or glues in combination with a conductive coating)</li></ul></li><li><p>Sample insertion</p></li><li><p>Evacuation</p></li></ul><h4 id="accelerating-voltage">Accelerating voltage</h4><figure><img src="/2019/01/11/Microscopy/Users/xsx/Desktop/source/photos/figure_33.svg" alt="figure_33"><figcaption>figure_33</figcaption></figure><figure><img src="/2019/01/11/Microscopy/Users/xsx/Desktop/source/photos/figure_34.svg" alt="figure_34"><figcaption>figure_34</figcaption></figure><h4 id="apertures">Apertures</h4><p>An <strong>aperture</strong> is a minute hole in a strip of metal that is placed in the path of the electron beam in order to restrict or limit electron progress down the machine column.</p><figure><img src="/2019/01/11/Microscopy/Users/xsx/Desktop/source/photos/figure_36.jpg" alt="figure_36"><figcaption>figure_36</figcaption></figure><ul><li>large aperture for low-magnification to increase signal (BSE and microanalysis)</li><li>small aperture for high-resolution &amp; better depth of focus <strong>but</strong> fewer electrons =&gt; less bright image.</li></ul><figure><img src="/2019/01/11/Microscopy/Users/xsx/Desktop/source/photos/figure_37.svg" alt="figure_37"><figcaption>figure_37</figcaption></figure><p>Notes: <em>Wobbler control</em></p><h4 id="spot-size">Spot size</h4><ul><li>effect<ul><li>resolution</li><li>number of electrons</li><li>field of focus</li></ul></li><li>cause<ul><li>working distance smaller =&gt; smaller spot size</li><li>condenser lens setting higher current =&gt; smaller spot size</li><li>objective lens aperture smaller =&gt; smaller spot size</li></ul></li></ul><h4 id="working-distance">Working distance</h4><p><strong>Sample height</strong>, or <strong>working distance (WD)</strong>, refers to the distance between the bottom of the SEM column and the top of the sample.</p><ul><li><p>differences external Z control (mechanical control) vs WD</p></li><li><p>WD impacts</p><ul><li><p>depth of field</p></li><li><p>resolution</p><p><em>As the WD is increased the beam <strong>divergence angle</strong> is decreased which provides a greater depth of field.</em></p><p><em>The "trade-off" for an increased WD is that the electron beam must travel a greater distance from the gun and therefore has a larger spot size on the specimen.</em></p></li></ul></li></ul><figure><img src="/2019/01/11/Microscopy/Users/xsx/Desktop/source/photos/figure_41.svg" alt="figure_41"><figcaption>figure_41</figcaption></figure><h4 id="constrast-and-brightness">Constrast and brightness</h4><ul><li><p>Perfecting an image - Signal processing</p></li><li><p>Tilting to increase SE contrast</p></li></ul><h4 id="magnification-calibration">Magnification &amp; Calibration</h4><h4 id="scan-rate-signal-to-noise">Scan rate &amp; signal to noise</h4><ul><li><p>slower scan rate =&gt; more electrons at each point =&gt; better quality</p></li><li><p>quality limited</p><ul><li>spot size</li><li>signal(S)/noise(N)<ul><li>Noise &lt;= beam brightness, spot size, SE detector sensitivity =&gt; salt-and-pepper, grainy</li><li>high-resolution =&gt; low S/N ratio &amp; grainy.<br></li><li>increase electrons per point =&gt; improve S/N ratio</li><li>eg. Tungsten (W) filaments<ul><li>high resolution =&gt; small spot sizes =&gt; low quantity of electrons =&gt; low brightness =&gt; need high current detecter =&gt; low S/N ratio</li></ul></li></ul></li></ul></li></ul><h4 id="image-artefacts-and-trouble-shooting">Image artefacts and trouble-shooting</h4><ul><li><em>Astigmatism</em>: <strong>stigmator</strong></li><li>Lack of detail of surface structures: lower kVs (5-10kV)</li><li>Edge effects: lower kVs</li><li>Charging<ul><li>reduce electron energy(kV)</li><li>reduce number of electrons<ul><li>beam current</li><li>emission level of gun</li><li>spot size</li><li>apertures between gun &amp; specimen</li></ul></li><li>recoat the sample with a thicker layer of platinum</li></ul></li><li>Specimen damage<ul><li>lower beam energy</li><li>increase WD =&gt; larger spot size on the sample <strong>but</strong> reduce resolution</li></ul></li><li>Beam-related contamination<ul><li>take micrographs at low magnification</li><li>ensure sample clean</li></ul></li></ul><h3 id="specialised-sem-techniques">Specialised SEM techniques</h3><ul><li><p><em>CL</em></p><p>When an electron beam interacts with a luminescent material, visible light may be emitted as a result of electronic transitions in the band gap of the material.</p></li><li><p>ESEM:</p><ul><li>sample temperature</li><li>specimen chamber vapour pressure</li></ul></li><li><p>Cryo-SEM-Cold stage</p><p>hydrated (wet) samples, delicate biological samples, hydrogels, food, biofilms, foams, fats and waxes, suspensions, pharmaceuticals and nanoparticles</p></li><li><p>FIB</p><p>This technology involves using an ion beam (typically <strong>gallium</strong> ions) directed onto a hard sample</p></li><li><p>EDS</p><p>Energy dispersive X-ray spectroscopy (EDS or EDX) is an analytical technique used to investigate the elemental or chemical characteristics of a sample.</p></li><li><p>EBSD</p><p>Electron Backscatter Diffraction (EBSD) is a technique which allows the investigation of the structure, phase and crystal orientation of crystalline materials.</p></li><li><p><strong><em>EBL</em></strong></p><p><em>Electron beam lithography (EBL) is a maskless lithography technique used for patterning of computer generated layout structures on photoresists on silicon wafers.</em></p></li><li><p>Backscatter</p><ul><li><p><strong><em>electron channelling contrast imaging (ECCI)</em></strong></p><p><em>The change in diffraction of the backscattered electrons as they interact with a dislocation in the material results in a higher backscattering coefficient than for the matrix; so individual dislocations appear as bright lines in a darker matrix.</em></p></li><li><p>Coating</p><ul><li><p>uncoated for compositional information</p></li><li><p>non-conductive coated with carbon with compositional detail</p></li></ul></li><li><p>Topography and BSE</p><ul><li>flat =&gt; best compositional information</li><li>BSE image (greyscale differences &lt;= atomic number contrast)<ul><li>phases recognition &amp; classification</li><li>differences in elemental composition or concentrations</li></ul></li></ul></li></ul></li></ul><h2 id="transmission-electron-microscopy">Transmission electron microscopy</h2><h3 id="concepts">Concepts</h3><h4 id="resolution-1">Resolution：</h4><p><span class="math display">\[d=\frac{0.61\lambda}{nsin\alpha}\]</span></p><table><thead><tr class="header"><th style="text-align: left;">Where</th><th style="text-align: left;">Equals</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;"><span class="math inline">\(d\)</span></td><td style="text-align: left;">resolution (minimum resolvable distance)</td></tr><tr class="even"><td style="text-align: left;"><span class="math inline">\(\lambda\)</span></td><td style="text-align: left;">wavelength of the energy source</td></tr><tr class="odd"><td style="text-align: left;"><span class="math inline">\(n\)</span></td><td style="text-align: left;">refractive index of the medium</td></tr><tr class="even"><td style="text-align: left;"><span class="math inline">\(\alpha\)</span></td><td style="text-align: left;">aperture angle</td></tr></tbody></table><p>Note: The term <em>n</em>sin<em>α</em> is named numerical aperture.</p><h4 id="the-electron-wavelength">The electron wavelength</h4><h2 id="aberrations">aberrations</h2><ul><li><a href="https://myscope.training/legacy/tem/background/concepts/problems/spherical.php" target="_blank" rel="noopener">Spherical aberration</a></li><li><a href="https://myscope.training/legacy/tem/background/concepts/problems/chromatic.php" target="_blank" rel="noopener">Chromatic aberration</a></li><li><a href="https://myscope.training/legacy/tem/background/concepts/problems/astigmatism.php" target="_blank" rel="noopener">Astigmatism</a></li></ul><h3 id="background-information-1">Background information</h3><p>TEM 1μm ～1 nm</p><p>crystal structures, specimen orientations and chemical compositions of phases, precipitates and contaminants through diffraction pattern, X-ray and electron-energy analysis.</p><p>TEM vs LM(light microscope)</p><ol type="1"><li><em>Resolution at high magnification</em>. (The best resolution possible in a LM is about 200 nm whereas a typical TEM has a resolution of better than 1 nm.)</li><li><em>Structural information</em>.</li><li><em>Microanalysis</em></li></ol><h4 id="parts-of-the-machine">Parts of the machine</h4><h4 id="applications-and-practical-uses">Applications and practical uses</h4><p>Transmission electron microscopy <strong>can</strong>:</p><ul><li>Image morphology of samples, e.g. view sections of material, fine powders suspended on a thin film, small whole organisms such as viruses or bacteria, and frozen solutions.</li><li>Tilt a sample and collect a series of images to construct a 3-dimensional image.</li><li>Analyse the composition and some bonding differences (through <a href="https://myscope.training/legacy/tem/background/practical/#term" target="_blank" rel="noopener">contrast</a> and by using spectroscopy techniques: <a href="https://myscope.training/legacy/tem/background/practical/#term" target="_blank" rel="noopener">microanalysis</a>and electron energy loss).</li><li>Physically manipulate samples while viewing them, such as indent or compress them to measure mechanical properties (only when holders specialised for these techniques are available).</li><li>View frozen material (in a TEM with a cryostage).</li><li>Generate characteristic <a href="https://myscope.training/legacy/tem/background/practical/#term" target="_blank" rel="noopener">X-rays</a> from samples for microanalysis.</li><li>Acquire electron diffraction patterns (using the physics of Bragg Diffraction).</li><li>Perform electron energy loss spectroscopy of the beam passing through a sample to determine sample composition or the bonding states of atoms in the sample.</li></ul><p>There are some things TEM can't do:</p><ul><li>TEM cannot take colour images. Colour is sometimes added artificially to TEM images.</li><li>TEM cannot image through thick samples: the usual sample thickness is around 100-200nm. <a href="https://myscope.training/legacy/tem/background/practical/temcannot.php#term" target="_blank" rel="noopener">Electron</a>s cannot readily penetrate sections much thicker than 200nm.</li><li>A standard TEM cannot image surface information.</li><li>The TEM cannot reliably image charged molecules that are mobile in a matrix. For example, some species (e.g. Na+) are volatile under the <a href="https://myscope.training/legacy/tem/background/practical/temcannot.php#term" target="_blank" rel="noopener">electron beam</a> because the negative electron beam exerts a force on charged material.</li></ul><h3 id="transmission-electron-microscopy-in-practice">Transmission electron microscopy in practice</h3><h4 id="sample-preparation">Sample preparation</h4><h4 id="machine-settings">Machine settings</h4><h4 id="alignment">Alignment</h4><ol type="1"><li>Illumination system: beam tilt; beam shift</li><li>Condenser aperture alignment</li><li>Condenser astigmatism correction</li><li>Alignment of illumination with respect to the objective lens</li><li>Objective aperture centering</li><li>Objective astigmatism correction</li><li>Alignment of intermediate lens</li></ol><h2 id="x-ray-diffraction">x-ray diffraction</h2><h2 id="scan.prob-atomic-force-microscopy">Scan.prob &amp; atomic force microscopy</h2><h2 id="light-microscopy">Light microscopy</h2><h2 id="microanalysis">Microanalysis</h2><p>spatial resolution &amp; detection limit</p><figure><img src="/2019/01/11/Microscopy/bubblechart.png" alt="bubblechart"><figcaption>bubblechart</figcaption></figure><h2 id="atom-probe-tomography">Atom probe tomography</h2><h2 id="work-health-and-safety">Work health and safety</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MyScope was developed by Microscopy Australia to provide an online learning environment for those who want to learn about microscopy. The platform provides insights into the fundamental science behind different microscopes, explores what can and cannot be measured by different systems and provides a realistic operating experience on high end microscopes.&lt;/p&gt;
    
    </summary>
    
      <category term="Materials characterization" scheme="https://github-9233.github.io/categories/Materials-characterization/"/>
    
    
  </entry>
  
</feed>
