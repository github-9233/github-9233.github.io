<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">













  <meta name="google-site-verification" content="googleddad1e65378094ec.html">














  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.6.0" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.6.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.6.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.6.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.6.0" color="#222">









<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.6.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Machine learning is the study that allows computers to adaptively improve their performance with experience accumulated from the data observed. Our two sister courses teach the most fundamental algori">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning Foundations &amp; Techniques">
<meta property="og:url" content="https://github-9233.github.io/2019/01/17/Machine Learning-Hsuan-Tien Lin/index.html">
<meta property="og:site_name" content="Leon Xu&#39;s Notes">
<meta property="og:description" content="Machine learning is the study that allows computers to adaptively improve their performance with experience accumulated from the data observed. Our two sister courses teach the most fundamental algori">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2019-04-11T02:38:17.634Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Machine Learning Foundations &amp; Techniques">
<meta name="twitter:description" content="Machine learning is the study that allows computers to adaptively improve their performance with experience accumulated from the data observed. Our two sister courses teach the most fundamental algori">



  <link rel="alternate" href="/atom.xml" title="Leon Xu's Notes" type="application/atom+xml">




  <link rel="canonical" href="https://github-9233.github.io/2019/01/17/Machine Learning-Hsuan-Tien Lin/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Machine Learning Foundations & Techniques | Leon Xu's Notes</title>
  




<script async src="https://www.googletagmanager.com/gtag/js?id=UA-159134835-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-159134835-1');
</script>









  <noscript>
  <style>
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion .logo-line-before i { left: initial; }
    .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Leon Xu's Notes</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-sitemap">

    
    
    
      
    

    

    <a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br>Sitemap</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    
  
  
  
  

  

    <a href="https://github.com/github-9233/" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" style="fill: #222; color: #fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a>



    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github-9233.github.io/2019/01/17/Machine Learning-Hsuan-Tien Lin/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Leon Xu">
      <meta itemprop="description" content="True knowledge exists in knowing that you know nothing.">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Leon Xu's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Machine Learning Foundations & Techniques

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            

            
              

              
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                <time title="Modified: 2019-04-11 10:38:17" itemprop="dateModified" datetime="2019-04-11T10:38:17+08:00">2019-04-11</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/Computer-science/" itemprop="url" rel="index"><span itemprop="name">Computer science</span></a></span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/Computer-science/Machine-learning/" itemprop="url" rel="index"><span itemprop="name">Machine learning</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/01/17/Machine Learning-Hsuan-Tien Lin/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">Comments: </span> <span class="post-comments-count valine-comment-count" data-xid="/2019/01/17/Machine Learning-Hsuan-Tien Lin/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2019/01/17/Machine Learning-Hsuan-Tien Lin/" class="leancloud_visitors" data-flag-title="Machine Learning Foundations & Techniques">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-symbolscount">
              

              

              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">13 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Machine learning is the study that allows computers to adaptively improve their performance with experience accumulated from the data observed. Our two sister courses teach the most fundamental algorithmic, theoretical and practical tools that any user of machine learning needs to know. This first course of the two would focus more on mathematical tools, and the other course would focus more on algorithmic tools.</p>
<a id="more"></a>
<h1 id="machine-learning-foundations"><a href="https://www.csie.ntu.edu.tw/~htlin/course/" target="_blank" rel="noopener">Machine Learning Foundations</a></h1>
<h2 id="topic-1-when-can-machines-learn">Topic 1: When can machines learn?</h2>
<p>###Lecture 1: The Learning Problem</p>
<p><em>A</em> takes <em>D</em> and <em>H</em> to get <em>g</em></p>
<ul>
<li><p>Course Introduction</p>
<p>foundation oriented and story-like</p></li>
<li><p>What is Machine Learning</p>
<p>use data to approximate target</p></li>
<li><p>Applications of Machine Learning</p>
<p>almost everywhere</p></li>
<li><p>Components of Machine Learning</p>
<p><em>A</em> takes <em>D</em> and <em>H</em> to get <em>g</em></p></li>
<li><p>Machine Learning and Other Fields</p>
<p>related to Date Mining, Artificial Intelligence and Statstics</p></li>
</ul>
<h3 id="lecture-2-learning-to-answer-yesno">Lecture 2: Learning to Answer Yes/No</h3>
<p>PLA <em>A</em> takes linear separable <em>D</em> and perceptrons <em>H</em> to get hypothesis <em>g</em></p>
<ul>
<li><p>Perceptron Hypothesis Set</p>
<p>hyperplanes/linear classifiers in Rd</p></li>
<li><p>Perceptron Learning Algorithm (PLA)</p>
<p>correct mistakes and improve iteratively</p></li>
<li><p>Guarantee of PLA</p>
<p>no mistake eventually if linear separable</p></li>
<li><p>Non-Separable Data</p>
<p>hold somewhat ‘best’ weights in pocket</p></li>
</ul>
<h3 id="lecture-3-types-of-learning">Lecture 3: Types of Learning</h3>
<p>focus: binary classification or regression from a batch of supervised data with concrete features</p>
<ul>
<li><p>Learning with Different Output Space Y</p>
<p>[classification], [regression], structured</p></li>
<li><p>Learning with Different Data Label yn</p>
<p>[supervised], un/semi-supervised, reinforcement</p></li>
<li><p>Learning with Different Protocol f ⇒ (xn, yn)</p>
<p>[batch], online, active</p></li>
<li><p>Learning with Different Input Space X</p>
<p>[concrete], raw, abstract</p></li>
</ul>
<h3 id="lecture-4-feasibility-of-learning">Lecture 4: feasibility of learning</h3>
<p>learning is PAC-possible if enough statistical data and finite |H|</p>
<ul>
<li><p>Learning is Impossible?</p>
<p>absolutely no free lunch outside D</p></li>
<li><p>Probability to the Rescue</p>
<p>probably approximately correct outside D</p></li>
<li><p>Connection to Learning</p>
<p>verification possible if Ein(h) small for fixed h</p></li>
<li><p>Connection to Real Learning</p>
<p>learning possible if |H| finite and Ein(g) small</p></li>
</ul>
<h2 id="topic-2-why-can-machines-learn">Topic 2: Why Can Machines Learn?</h2>
<h3 id="lecture-5-training-versus-testing">Lecture 5: training versus testing</h3>
<p>effective price of choice in training: (wishfully) growth function mH(N) with a break point</p>
<ul>
<li><p>Recap and Preview</p>
<p>two questions: Eout(g) ≈ Ein(g), and Ein(g) ≈ 0</p></li>
<li><p>Effective Number of Lines</p>
<p>at most 14 through the eye of 4 inputs</p></li>
<li><p>Effective Number of Hypotheses</p>
<p>at most mH(N) through the eye of N inputs</p></li>
<li><p>Break Point</p>
<p>when mH(N) becomes ‘non-exponential’</p></li>
</ul>
<h3 id="lecture-6-theory-of-generalization">Lecture 6: theory of generalization</h3>
<p>Eout ≈ Ein possible if mH(N) breaks somewhere and N large enough</p>
<ul>
<li><p>Restriction of Break Point</p>
<p>break point ‘breaks’ consequent points</p></li>
<li><p>Bounding Function: Basic Cases</p>
<p>B(N,k) bounds mH(N) with break point k</p></li>
<li><p>Bounding Function: Inductive Cases</p>
<p>B(N,k) is poly(N)</p></li>
<li><p>A Pictorial Proof</p>
<p>mH(N) can replace M with a few changes</p></li>
</ul>
<h3 id="lecture-7-the-vc-dimension">Lecture 7: the VC dimension</h3>
<p>learning happens if finite dVC, large N, and low Ein</p>
<ul>
<li><p>Definition of VC Dimension</p>
<p>maximum non-break point</p></li>
<li><p>VC Dimension of Perceptrons</p>
<p>dVC(H) = d + 1</p></li>
<li><p>Physical Intuition of VC Dimension</p>
<p>dVC ≈ #free parameters</p></li>
<li><p>Interpreting VC Dimension</p>
<p>loosely: model complexity &amp; sample complexity</p></li>
</ul>
<h3 id="lecture-8-noise-and-error">Lecture 8: noise and error</h3>
<p>learning can happen with target distribution P(y|x) and low Ein w.r.t. err</p>
<ul>
<li><p>Noise and Probabilistic Target</p>
<p>can replace f(x) by P(y|x)</p></li>
<li><p>Error Measure</p>
<p>affect ‘ideal’ target</p></li>
<li><p>Algorithmic Error Measure</p>
<p>user-dependent =⇒ plausible or friendly</p></li>
<li><p>Weighted Classification</p>
<p>easily done by virtual ‘example copying’</p></li>
</ul>
<h2 id="topic-3-how-can-machines-learn">Topic 3: How Can Machines Learn?</h2>
<h3 id="lecture-9-linear-regression">Lecture 9: linear regression</h3>
<p>analytic solution wLIN = X†y with linear regression hypotheses and squared error</p>
<ul>
<li><p>Linear Regression Problem</p>
<p>use hyperplanes to approximate real values</p></li>
<li><p>Linear Regression Algorithm</p>
<p>analytic solution with pseudo-inverse</p></li>
<li><p>Generalization Issue</p>
<p>Eout − Ein ≈ 2(d+1) on average N</p></li>
<li><p>Linear Regression for Binary Classification</p>
<p>0/1 error ≤ squared error</p></li>
</ul>
<h3 id="lecture-10-logistic-regression">Lecture 10: logistic regression</h3>
<p>gradient descent on cross-entropy error to get good logistic hypothesis</p>
<ul>
<li><p>Logistic Regression Problem</p>
<p>P(+1|x) as target and θ(wT x) as hypotheses</p></li>
<li><p>Logistic Regression Error</p>
<p>cross-entropy (negative log likelihood)</p></li>
<li><p>Gradient of Logistic Regression Error</p>
<p>θ-weighted sum of data vectors</p></li>
<li><p>Gradient Descent</p>
<p>roll downhill by −∇Ein(w)</p></li>
</ul>
<h3 id="lecture-11-linear-models-for-classification">Lecture 11: linear models for classification</h3>
<p>binary classification via (logistic) regression; multiclass via OVA/OVO decomposition</p>
<ul>
<li><p>Linear Models for Binary Classification</p>
<p>three models useful in different ways</p></li>
<li><p>Stochastic Gradient Descent</p>
<p>follow negative stochastic gradient</p></li>
<li><p>Multiclass via Logistic Regression</p>
<p>predict with maximum estimated P(k|x)</p></li>
<li><p>Multiclass via Binary Classification</p>
<p>predict the tournament champion</p></li>
</ul>
<h3 id="lecture-12-nonlinear-transformation">Lecture 12: nonlinear transformation</h3>
<p>nonlinear X􏰡 via nonlinear feature transform Φ plus linear X with price of model complexity</p>
<ul>
<li><p>Quadratic Hypotheses</p>
<p>linear hypotheses on quadratic-transformed data</p></li>
<li><p>Nonlinear Transform</p>
<p>happy linear modeling after Z = Φ(X )</p></li>
<li><p>Price of Nonlinear Transform</p>
<p>computation/storage/[model complexity]</p></li>
<li><p>Structured Hypothesis Sets</p>
<p>linear/simpler model first</p></li>
</ul>
<h2 id="topic-4-how-can-machines-learn-better">Topic 4: How Can Machines Learn Better?</h2>
<h3 id="lecture-13-hazard-of-overfitting">Lecture 13: hazard of overfitting</h3>
<p>overfitting happens with excessive power, stochastic/deterministic noise, and limited data</p>
<ul>
<li><p>What is Overfitting?</p>
<p>lower Ein but higher Eout</p></li>
<li><p>The Role of Noise and Data Size</p>
<p>overfitting ‘easily’ happens!</p></li>
<li><p>Deterministic Noise</p>
<p>what H cannot capture acts like noise</p></li>
<li><p>Dealing with Overfitting</p>
<p>data cleaning/pruning/hinting, and more</p></li>
</ul>
<h3 id="lecture-14-regularization">Lecture 14: regularization</h3>
<p>minimizes augmented error, where the added regularizer effectively limits model complexity</p>
<ul>
<li><p>Regularized Hypothesis Set</p>
<p>original H + constraint</p></li>
<li><p>Weight Decay Regularization</p>
<p>add λ/N*wTw in Eaug</p></li>
<li><p>Regularization and VC Theory</p>
<p>regularization decreases dEFF</p></li>
<li><p>General Regularizers</p>
<p>target-dependent, [plausible], or [friendly]</p></li>
</ul>
<h3 id="lecture-15-validation">Lecture 15: validation</h3>
<p>(crossly) reserve validation data to simulate testing procedure for model selection</p>
<ul>
<li><p>Model Selection Problem</p>
<p>dangerous by Ein and dishonest by Etest</p></li>
<li><p>Validation</p>
<p>select with Eval(Am(Dtrain)) while returning Am∗ (D)</p></li>
<li><p>Leave-One-Out Cross Validation</p>
<p>huge computation for almost unbiased estimate</p></li>
<li><p>V -Fold Cross Validation</p>
<p>reasonable computation and performance</p></li>
</ul>
<h3 id="lecture-16-three-learning-principle">Lecture 16: three learning principle</h3>
<ul>
<li><p>Occam’s Razor</p>
<p>simple, simple, simple!</p></li>
<li><p>Sampling Bias</p>
<p>match test scenario as much as possible</p></li>
<li><p>Data Snooping</p>
<p>any use of data is ‘contamination’</p></li>
<li><p><strong>Power of Three</strong></p>
<p>relatives, bounds, models, tools, principles</p></li>
</ul>
<h1 id="machine-learning-techniques">Machine Learning Techniques</h1>
<h2 id="topic-1-embedding-numerous-features-kernel-models">Topic 1: Embedding Numerous Features: Kernel Models</h2>
<h3 id="lecture-1-linear-support-vector-machine">Lecture 1: Linear Support Vector Machine</h3>
<p>linear SVM: more robust and solvable with quadratic programming</p>
<ul>
<li><p>Course Introduction</p>
<p>from foundations to techniques</p></li>
<li><p>Large-Margin Separating Hyperplane</p>
<p>intuitively more robust against noise</p></li>
<li><p>Standard Large-Margin Problem</p>
<p>minimize ‘length of w’ at special separating scale</p></li>
<li><p>Support Vector Machine</p>
<p>‘easy’ via quadratic programming</p></li>
<li><p>Reasons behind Large-Margin Hyperplane</p>
<p>fewer dichotomies and better generalization</p></li>
</ul>
<h3 id="lecture-2-dual-support-vector-machine">Lecture 2: Dual Support Vector Machine</h3>
<p>dual SVM: another QP with valuable geometric messages and almost no dependence on d ̃</p>
<ul>
<li><p>Motivation of Dual SVM</p>
<p>want to remove dependence on d ̃</p></li>
<li><p>Lagrange Dual SVM</p>
<p>KKT conditions link primal/dual</p></li>
<li><p>Solving Dual SVM</p>
<p>another QP, better solved with special solver</p></li>
<li><p>Messages behind Dual SVM</p>
<p>SVs represent fattest hyperplane</p></li>
</ul>
<h3 id="lecture-3-kernel-support-vector-machine">Lecture 3: Kernel Support Vector Machine</h3>
<p>kernel as a shortcut to (transform + inner product) to remove dependence on d ̃: allowing a spectrum of simple (linear) models to infinite dimensional (Gaussian) ones with margin control</p>
<ul>
<li><p>Kernel Trick</p>
<p>kernel as shortcut of transform + inner product</p></li>
<li><p>Polynomial Kernel</p>
<p>embeds specially-scaled polynomial transform</p></li>
<li><p>Gaussian Kernel</p>
<p>embeds infinite dimensional transform</p></li>
<li><p>Comparison of Kernels</p>
<p>linear for efficiency or Gaussian for power</p></li>
</ul>
<h3 id="lecture-4-soft-margin-support-vector-machine">Lecture 4: Soft-Margin Support Vector Machine</h3>
<p>allow some margin violations ξn while penalizing them by C; equivalent to upper-bounding αn by C</p>
<ul>
<li><p>Motivation and Primal Problem</p>
<p>add margin violations ξn</p></li>
<li><p>Dual Problem</p>
<p>upper-bound αn by C</p></li>
<li><p>Messages behind Soft-Margin SVM</p>
<p>bounded/free SVs for data analysis</p></li>
<li><p>Model Selection</p>
<p>cross-validation, or approximately nSV</p></li>
</ul>
<h3 id="lecture-5-kernel-logistic-regression">Lecture 5: Kernel Logistic Regression</h3>
<p>two-level learning for SVM-like sparse model for soft classification, or using representer theorem with regularized logistic error for dense model</p>
<ul>
<li><p>Soft-Margin SVM as Regularized Model</p>
<p>L2-regularization with hinge error measure</p></li>
<li><p>SVM versus Logistic Regression</p>
<p>≈ L2-regularized logistic regression</p></li>
<li><p>SVM for Soft Binary Classification</p>
<p>common approach: two-level learning</p></li>
<li><p>Kernel Logistic Regression</p>
<p>representer theorem on L2-regularized LogReg</p></li>
</ul>
<h3 id="lecture-6-support-vector-regression">Lecture 6: Support Vector Regression</h3>
<p>kernel ridge regression (dense) via ridge regression + representer theorem; support vector regression (sparse) via regularized tube error + Lagrange dual</p>
<ul>
<li><p>Kernel Ridge Regression</p>
<p>representer theorem on ridge regression</p></li>
<li><p>Support Vector Regression Primal</p>
<p>minimize regularized tube errors</p></li>
<li><p>Support Vector Regression Dual</p>
<p>a QP similar to SVM dual</p></li>
<li><p>Summary of Kernel Models</p>
<p>with great power comes great responsibility</p></li>
</ul>
<h2 id="topic-2-combining-predictive-features-aggregation-models">Topic 2: Combining Predictive Features: Aggregation Models</h2>
<h3 id="lecture-7-blending-and-bagging">Lecture 7: Blending and Bagging</h3>
<p>blending known diverse hypotheses uniformly, linearly, or even non-linearly; obtaining diverse hypotheses from bootstrapped data</p>
<ul>
<li><p>Motivation of Aggregation</p>
<p>aggregated G strong and/or moderate</p></li>
<li><p>Uniform Blending</p>
<p>diverse hypotheses, ‘one vote, one value’</p></li>
<li><p>Linear and Any Blending</p>
<p>two-level learning with hypotheses as transform</p></li>
<li><p>Bagging (Bootstrap Aggregation)</p>
<p>bootstrapping for diverse hypotheses</p></li>
</ul>
<h3 id="lecture-8-adaptive-boosting">Lecture 8: Adaptive Boosting</h3>
<p>optimal re-weighting for diverse hypotheses and adaptive linear aggregation to boost ‘weak’ algorithms</p>
<ul>
<li><p>Motivation of Boosting</p>
<p>aggregate weak hypotheses for strength</p></li>
<li><p>Diversity by Re-weighting</p>
<p>scale up incorrect, scale down correct</p></li>
<li><p>Adaptive Boosting Algorithm</p>
<p>two heads are better than one, theoretically</p></li>
<li><p>Adaptive Boosting in Action</p>
<p>AdaBoost-Stump useful and efficient</p></li>
</ul>
<h3 id="lecture-9-decision-tree">Lecture 9: Decision Tree</h3>
<p>recursive branching (purification) for conditional aggregation of constant hypotheses</p>
<ul>
<li><p>Decision Tree Hypothesis</p>
<p>express path-conditional aggregation</p></li>
<li><p>Decision Tree Algorithm</p>
<p>recursive branching until termination to base</p></li>
<li><p>Decision Tree Heuristics in C&amp;RT</p>
<p>pruning, categorical branching, surrogate</p></li>
<li><p>Decision Tree in Action</p>
<p>explainable and efficient</p></li>
</ul>
<h3 id="lecture-10-random-forest">Lecture 10: Random Forest</h3>
<p>bagging of randomized C&amp;RT trees with automatic validation and feature selection</p>
<ul>
<li><p>Random Forest Algorithm</p>
<p>bag of trees on randomly projected subspaces</p></li>
<li><p>Out-Of-Bag Estimate</p>
<p>self-validation with OOB examples</p></li>
<li><p>Feature Selection</p>
<p>permutation test for feature importance</p></li>
<li><p>Random Forest in Action</p>
<p>‘smooth’ boundary with many trees</p></li>
</ul>
<h3 id="lecture-11-gradient-boosted-decision-tree">Lecture 11: Gradient Boosted Decision Tree</h3>
<p>aggregating trees from functional gradient and steepest descent subject to any error measure</p>
<ul>
<li><p>Adaptive Boosted Decision Tree</p>
<p>sampling and pruning for ‘weak’ trees</p></li>
<li><p>Optimization View of AdaBoost</p>
<p>functional gradient descent on exponential error</p></li>
<li><p>Gradient Boosting</p>
<p>iterative steepest residual fitting</p></li>
<li><p>Summary of Aggregation Models</p>
<p>some cure underfitting; some cure overfitting</p></li>
</ul>
<h2 id="topic-3-distilling-implicit-features-extraction-models">Topic 3: Distilling Implicit Features: Extraction Models</h2>
<h3 id="lecture-12-neural-network">Lecture 12: Neural Network</h3>
<p>automatic pattern feature extraction from layers of neurons with backprop for GD/SGD</p>
<ul>
<li><p>Motivation</p>
<p>multi-layer for power with biological inspirations</p></li>
<li><p>Neural Network Hypothesis</p>
<p>layered pattern extraction until linear hypothesis</p></li>
<li><p>Neural Network Learning</p>
<p>backprop to compute gradient efficiently</p></li>
<li><p>Optimization and Regularization</p>
<p>tricks on initialization, regularizer, early stopping</p></li>
</ul>
<h3 id="lecture-13-deep-learning">Lecture 13: Deep Learning</h3>
<p>pre-training with denoising autoencoder (non-linear PCA) and fine-tuning with backprop for NNet with many layers</p>
<ul>
<li><p>Deep Neural Network</p>
<p>difficult hierarchical feature extraction problem</p></li>
<li><p>Autoencoder</p>
<p>unsupervised NNet learning of representation</p></li>
<li><p>Denoising Autoencoder</p>
<p>using noise as hints for regularization</p></li>
<li><p>Principal Component Analysis</p>
<p>linear autoencoder variant for data processing</p></li>
</ul>
<h3 id="lecture-14-radial-basis-function-network">Lecture 14: Radial Basis Function Network</h3>
<p>linear aggregation of distance-based similarities using k-Means clustering for prototype finding</p>
<ul>
<li>RBF Network Hypothesis</li>
</ul>
<p>prototypes instead of neurons as transform</p>
<ul>
<li>RBF Network Learning</li>
</ul>
<p>linear aggregation of prototype ‘hypotheses’</p>
<ul>
<li>k-Means Algorithm</li>
</ul>
<p>clustering with alternating optimization</p>
<ul>
<li>k-Means and RBF Network in Action</li>
</ul>
<p>proper choice of # prototypes important</p>
<h3 id="lecture-15-matrix-factorization">Lecture 15: Matrix Factorization</h3>
<p>linear models of movies on extracted user features (or vice versa) jointly optimized with stochastic gradient descent</p>
<ul>
<li>Linear Network Hypothesis</li>
</ul>
<p>feature extraction from binary vector encoding</p>
<ul>
<li>Basic Matrix Factorization</li>
</ul>
<p>alternating least squares between user/movie</p>
<ul>
<li>Stochastic Gradient Descent</li>
</ul>
<p>efficient and easily modified for practical use</p>
<ul>
<li>Summary of Extraction Models</li>
</ul>
<p>powerful thus need careful use</p>
<h3 id="lecture-16-finale">Lecture 16: Finale</h3>
<ul>
<li><p>Feature Exploitation Techniques</p>
<p>kernel, aggregation, extraction, low-dimensional</p></li>
<li><p>Error Optimization Techniques</p>
<p>gradient, equivalence, stages</p></li>
<li><p>Overfitting Elimination Techniques</p>
<p>(lots of) regularization, validation</p></li>
<li><p>Machine Learning in Practice</p>
<p>welcome to the jungle</p></li>
</ul>
<h1 id="notes">Notes</h1>
<ul>
<li><p>Terms</p>
<ul>
<li>正则 岭回归</li>
<li>伪逆矩阵</li>
<li>离散函数</li>
<li>拉格朗日对偶法</li>
<li>拉格朗日最小化</li>
<li>拉格朗日乘法</li>
<li>空间转换</li>
<li>原始SVM</li>
<li>对偶SVM</li>
<li>Decision Stump</li>
</ul></li>
<li><p>L1-5 25</p></li>
<li><p>L2-19</p></li>
<li><p>L4-18 19</p></li>
<li><p>L5-4 5 9 15 17</p></li>
<li><p>L7基于以上的两个例子，我们得到了aggregation的两个优势：feature transform和regularization。我们之前在机器学习基石课程中就介绍过，feature transform和regularization是对立的，还把它们分别比作踩油门和踩刹车。如果进行feature transform，那么regularization的效果通常很差，反之亦然。也就是说，单一模型通常只能倾向于feature transform和regularization之一，在两者之间做个权衡。但是aggregation却能将feature transform和regularization各自的优势结合起来，好比把油门和刹车都控制得很好，从而得到不错的预测模型。</p></li>
<li><p>L07-20 bootstrap aggregation (BAGging): a simple meta algorithm on top of base algorithm A</p></li>
<li><p>L07-21 值得注意的是，只有当演算法对数据样本分布比较敏感的情况下，才有比较好的表现。</p></li>
<li><p>L08-20</p></li>
<li><p>L09-10</p></li>
<li><p>L10-2Bagging能减小variance，而Decision Tree能增大variance</p></li>
<li><p>L11-10</p></li>
<li><p>L12-6感知机模型每个节点的输入就对应神经元的树突dendrite，感知机每个节点的输出就对应神经元的轴突axon</p></li>
<li><p>L12-13 16GD 21</p></li>
<li><p>L14-5 9 10</p></li>
</ul>

      
    </div>

    

    
    
    

    

    
      
    
    

    
      <div>
        



  



<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>Leon Xu</li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    
    <a href="https://github-9233.github.io/2019/01/17/Machine Learning-Hsuan-Tien Lin/" title="Machine Learning Foundations & Techniques">https://github-9233.github.io/2019/01/17/Machine Learning-Hsuan-Tien Lin/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.</li>
</ul>

      </div>
    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/04/13/MicroscopyBasics/" rel="prev" title="MicroscopyBasics">
                MicroscopyBasics <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.gif" alt="Leon Xu">
            
              <p class="site-author-name" itemprop="name">Leon Xu</p>
              <p class="site-description motion-element" itemprop="description">True knowledge exists in knowing that you know nothing.</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives">
                
                    <span class="site-state-item-count">12</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">2</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
             <div class="cc-license motion-element" itemprop="license">
              
                
              
              
              
              <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
             </div>
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.3blue1brown.com" title="https://www.3blue1brown.com" rel="noopener" target="_blank">3blue1brown</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://news.ycombinator.com" title="https://news.ycombinator.com" rel="noopener" target="_blank">hacker news</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://program-think.blogspot.com" title="https://program-think.blogspot.com" rel="noopener" target="_blank">program-think</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.susanjfowler.com/blog/2016/8/13/so-you-want-to-learn-physics" title="https://www.susanjfowler.com/blog/2016/8/13/so-you-want-to-learn-physics" rel="noopener" target="_blank">Susan Fowler's guide for learning physics</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#machine-learning-foundations"><span class="nav-text">Machine Learning Foundations</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#topic-1-when-can-machines-learn"><span class="nav-text">Topic 1: When can machines learn?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-2-learning-to-answer-yesno"><span class="nav-text">Lecture 2: Learning to Answer Yes/No</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-3-types-of-learning"><span class="nav-text">Lecture 3: Types of Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-4-feasibility-of-learning"><span class="nav-text">Lecture 4: feasibility of learning</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#topic-2-why-can-machines-learn"><span class="nav-text">Topic 2: Why Can Machines Learn?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-5-training-versus-testing"><span class="nav-text">Lecture 5: training versus testing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-6-theory-of-generalization"><span class="nav-text">Lecture 6: theory of generalization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-7-the-vc-dimension"><span class="nav-text">Lecture 7: the VC dimension</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-8-noise-and-error"><span class="nav-text">Lecture 8: noise and error</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#topic-3-how-can-machines-learn"><span class="nav-text">Topic 3: How Can Machines Learn?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-9-linear-regression"><span class="nav-text">Lecture 9: linear regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-10-logistic-regression"><span class="nav-text">Lecture 10: logistic regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-11-linear-models-for-classification"><span class="nav-text">Lecture 11: linear models for classification</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-12-nonlinear-transformation"><span class="nav-text">Lecture 12: nonlinear transformation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#topic-4-how-can-machines-learn-better"><span class="nav-text">Topic 4: How Can Machines Learn Better?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-13-hazard-of-overfitting"><span class="nav-text">Lecture 13: hazard of overfitting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-14-regularization"><span class="nav-text">Lecture 14: regularization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-15-validation"><span class="nav-text">Lecture 15: validation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-16-three-learning-principle"><span class="nav-text">Lecture 16: three learning principle</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#machine-learning-techniques"><span class="nav-text">Machine Learning Techniques</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#topic-1-embedding-numerous-features-kernel-models"><span class="nav-text">Topic 1: Embedding Numerous Features: Kernel Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-1-linear-support-vector-machine"><span class="nav-text">Lecture 1: Linear Support Vector Machine</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-2-dual-support-vector-machine"><span class="nav-text">Lecture 2: Dual Support Vector Machine</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-3-kernel-support-vector-machine"><span class="nav-text">Lecture 3: Kernel Support Vector Machine</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-4-soft-margin-support-vector-machine"><span class="nav-text">Lecture 4: Soft-Margin Support Vector Machine</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-5-kernel-logistic-regression"><span class="nav-text">Lecture 5: Kernel Logistic Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-6-support-vector-regression"><span class="nav-text">Lecture 6: Support Vector Regression</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#topic-2-combining-predictive-features-aggregation-models"><span class="nav-text">Topic 2: Combining Predictive Features: Aggregation Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-7-blending-and-bagging"><span class="nav-text">Lecture 7: Blending and Bagging</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-8-adaptive-boosting"><span class="nav-text">Lecture 8: Adaptive Boosting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-9-decision-tree"><span class="nav-text">Lecture 9: Decision Tree</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-10-random-forest"><span class="nav-text">Lecture 10: Random Forest</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-11-gradient-boosted-decision-tree"><span class="nav-text">Lecture 11: Gradient Boosted Decision Tree</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#topic-3-distilling-implicit-features-extraction-models"><span class="nav-text">Topic 3: Distilling Implicit Features: Extraction Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-12-neural-network"><span class="nav-text">Lecture 12: Neural Network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-13-deep-learning"><span class="nav-text">Lecture 13: Deep Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-14-radial-basis-function-network"><span class="nav-text">Lecture 14: Radial Basis Function Network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-15-matrix-factorization"><span class="nav-text">Lecture 15: Matrix Factorization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lecture-16-finale"><span class="nav-text">Lecture 16: Finale</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#notes"><span class="nav-text">Notes</span></a></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 – <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Leon Xu</span>

  

  
</div>


  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v6.6.0</div>



  <div class="footer-custom">Hosted by <a href="https://pages.github.com" class="theme-link" rel="noopener" target="_blank">GitHub Pages</a></div>


        








        
      </div>
    </footer>

    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>












  





  
  











  
  
    <script src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script src="/lib/three/three.min.js"></script>
  

  
  
    <script src="/lib/three/three-waves.min.js"></script>
  


  


  <script src="/js/src/utils.js?v=6.6.0"></script>

  <script src="/js/src/motion.js?v=6.6.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.6.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.6.0"></script>



  
  <script src="/js/src/scrollspy.js?v=6.6.0"></script>
<script src="/js/src/post-details.js?v=6.6.0"></script>



  


  <script src="/js/src/bootstrap.js?v=6.6.0"></script>



  



  








  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  
  
    
  
  <script src="https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js"></script>

  <script>
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(function (item) {
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: yes,
        notify: false,
        appId: 'mH0ReJE2w0tqGqQmFhciOMg5-MdYXbMMI',
        appKey: 'ztJcmG6TiVMUvtPF8C1Y6XvQ',
        placeholder: 'Just go go',
        avatar:'mm',
        meta:guest,
        pageSize:'10' || 10,
        visitor: true
    });
  </script>




  





  

  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<style>
.MathJax_Display {
    overflow: auto hidden;
}
</style>

    
  


  
  

  

  
  <script src="/js/src/js.cookie.js?v=6.6.0"></script>
  <script src="/js/src/scroll-cookie.js?v=6.6.0"></script>


  

  

  

  

</body>
</html>
